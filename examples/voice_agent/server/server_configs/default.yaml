# This is an example config for setting up a NeMo Voice Agent server.
# Please refer to https://github.com/NVIDIA-NeMo/NeMo/tree/main/examples/voice_agent/README.md for more details
# STT, LLM and TTS models have standalone configs in the folder "server/server_configs/{stt,llm,tts}_configs".
# Specify the type and an a model identifier to automatically configure the model.

transport:
  audio_out_10ms_chunks: 10  # use 4 as websocket default, but increasing to a larger number might have less glitches in TTS output

vad:
  type: silero
  confidence: 0.6  # VAD threshold for detecting speech versus non-speech
  start_secs: 0.1  # min amount of speech to trigger UserStartSpeaking
  stop_secs: 0.8  # min amount of silence to trigger UserStopSpeaking
  min_volume: 0.4  # Microphone volumn threshold for VAD

stt:
  type: nemo # choices in ['nemo'] currently only NeMo is supported
  model: "stt_en_fastconformer_hybrid_large_streaming_80ms"
  model_config: "./server_configs/stt_configs/nemo_cache_aware_streaming.yaml"
  device: "cuda"

diar:
  type: nemo
  enabled: true # set to false to disable
  model: "nvidia/diar_streaming_sortformer_4spk-v2"
  device: "cuda"
  threshold: 0.4  # threshold value used to determine if a speaker exists or not, setting it to a lower value will increaset the sensitivity of the model
  frame_len_in_secs: 0.08  # default for Sortformer, do not change unless using other architechtures

turn_taking:
  backchannel_phrases_path: "./server/backchannel_phrases.yaml"  # set it to the actual path of the file, or specify a list of backchannel phrases here
  max_buffer_size: 2  # num of words more than this amount will interrupt the LLM immediately if not backchannel phrases
  bot_stop_delay: 0.5  # a delay in seconds allowed between server and client audio output, so that the BotStopSpeaking signal is handled not too far away from the actual time that the user hears all audio output

llm:
  type: auto  # choices in ['auto', 'hf', 'vllm']
  model: "Qwen/Qwen2.5-7B-Instruct" # model name for HF models, will be used via `AutoModelForCausalLM.from_pretrained()`
  model_config: "./server_configs/llm_configs/qwen2.5-7B.yaml"
  # model: "nvidia/NVIDIA-Nemotron-Nano-9B-v2"  
  # model_config: "./server_configs/llm_configs/nemotron_nano_v2.yaml"
  # model: "Qwen/Qwen3-8B"
  # model_config: "./server_configs/llm_configs/qwen3-8B.yaml"
  device: "cuda"
  reasoning: false 

tts:
  type: nemo # choices in ['nemo', 'hf']
  model: fastpitch-hifigan
  model_config: "./server_configs/tts_configs/nemo_fastpitch-hifigan.yaml"
  device: "cuda"
